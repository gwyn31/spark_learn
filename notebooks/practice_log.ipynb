{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession, DataFrame as SparkDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "# 初始化spark session\n",
    "spark_session = SparkSession.builder.master(\"local[4]\").appName(\"test_log_processing\").getOrCreate()\n",
    "# standalone时不能把master线程数设的太高, 否则会因为Java GC的原因导致进程failed\n",
    "spark_session.conf.set(\"spark.driver.memory\", \"2g\")\n",
    "spark_session.conf.set(\"spark.executor.memory\", \"2g\")\n",
    "print(spark_session.version)  # 查看spark版本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过JDBC连接MySQL数据库, 读取数据\n",
    "# 由于数据库已经保留了表的schema, 因此不需要指定schema\n",
    "url = \"jdbc:mysql://localhost:3306\"  # JDBC URL\n",
    "table = \"test.fakelog\"  # 表名\n",
    "properties = {\"user\": \"root\", \"password\": \"z2026419\"}  # 连接属性(用户名和密码)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "记录数: 2336563\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "df = spark_session.read.jdbc(url, table, properties=properties)\n",
    "df.coalesce(50).persist(StorageLevel.MEMORY_AND_DISK)  \n",
    "# df的分区出现了一些问题, 下面代码可以正常输出前10行, 目前正在进一步了解分区与缓存的原理\n",
    "%time print(\"记录数: %d\" % df.count())  # 查看数据集的总记录数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-------+---------+--------------------+-----------+-------------------+\n",
      "|               LOGID|ACQ_NUM|DEQ_NUM|ACQ_SHARE|           ITEM_CODE|STATUS_FLAG|            LOGTIME|\n",
      "+--------------------+-------+-------+---------+--------------------+-----------+-------------------+\n",
      "|000005acf636cea98...|     19|      2|     0.82|5e0bqjonw8kmzya67...|          B|2019-05-18 08:02:51|\n",
      "|000011f2d737b56f6...|     16|     19|     0.41|  9o3ibs17824xkgazye|          B|2019-07-02 08:01:03|\n",
      "|000014a54e90d3af7...|      5|     16|     0.34|              7j0m8b|          B|2019-06-07 01:32:13|\n",
      "|00001b5238f8475b5...|     38|     18|     0.85|  8xb4zd6aok31ntw72p|          B|2019-08-03 14:39:00|\n",
      "|00001ead605d17b2a...|      8|     19|     0.67|          ijbra9o3tv|          A|2019-10-21 02:22:45|\n",
      "|000020f4a2388cb0b...|     34|      2|     0.97|        tl036rev7y42|          A|2019-07-08 11:16:53|\n",
      "|000021aa3f46fc60e...|     41|      8|     0.17| bhr5ocy807d41x3siqt|          B|2019-07-20 10:13:47|\n",
      "|00002963bfc216bfa...|     30|      1|     0.05|uo9dx5rhbp26kqwyc...|          B|2019-05-13 09:53:12|\n",
      "|0000490cb3c97206d...|      9|     16|     0.45|1bi685t2z0oxksqey...|          B|2019-01-13 21:53:21|\n",
      "|00004b48d7a5b89d9...|     15|      7|     0.46|  otzfleb7v4gd251cps|          A|2019-01-25 23:13:51|\n",
      "|00005259e730f5f0d...|     32|     20|     0.97|     4hibetumfx9rz15|          B|2019-10-09 05:18:49|\n",
      "|0000571274030d1bc...|     18|     20|     0.06|gxro4517qytz8wlcu...|          A|2019-07-10 04:17:11|\n",
      "|000064d5c8b8a0075...|     23|     17|     0.39|      0lu4pa3zfn9kjq|          B|2019-03-17 12:21:37|\n",
      "|00006692597db85db...|     18|     13|     0.37|          qlzpg5idmn|          A|2019-07-26 23:14:29|\n",
      "|00006c8475a22b2db...|     18|     14|     0.97|6cyais9u3k2wbt70r...|          A|2019-11-12 15:57:02|\n",
      "|000077ea0e26bdf89...|     13|     13|     0.16|            c0y9mpgh|          B|2019-10-26 08:28:34|\n",
      "|00007c4fe360632ed...|     14|      9|     0.90| 9ybovdl6tf3hp8g54in|          B|2019-01-10 01:27:18|\n",
      "|00007d981715fd8b9...|      3|     20|     0.55|ld4g30x5vrty9fwh6...|          B|2019-10-06 01:17:24|\n",
      "|00008cb8ec90ef6b4...|     26|     10|     0.45|          guob3jhk57|          A|2019-03-19 12:14:56|\n",
      "|00008d6da7d52cb44...|     16|     14|     0.60|67ob8xvpy1zq9rwsl...|          B|2019-03-14 15:07:30|\n",
      "+--------------------+-------+-------+---------+--------------------+-----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%time df.show()  # show的时间复杂度仿佛很高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 筛选与字段运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 筛选一定时间区间内的记录\n",
    "t1 = \"2019-05-06 12:32:56\"\n",
    "t2 = \"2019-06-06 12:32:56\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211410\n",
      "+--------------------+-------+-------+---------+--------------------+-----------+-------------------+\n",
      "|               LOGID|ACQ_NUM|DEQ_NUM|ACQ_SHARE|           ITEM_CODE|STATUS_FLAG|            LOGTIME|\n",
      "+--------------------+-------+-------+---------+--------------------+-----------+-------------------+\n",
      "|000005acf636cea98...|     19|      2|     0.82|5e0bqjonw8kmzya67...|          B|2019-05-18 08:02:51|\n",
      "|00002963bfc216bfa...|     30|      1|     0.05|uo9dx5rhbp26kqwyc...|          B|2019-05-13 09:53:12|\n",
      "|0000bc5270286f7a7...|     31|     15|     0.12|           hug4nvkwc|          B|2019-06-02 04:59:34|\n",
      "|000111c6c2c36620c...|     34|     19|     0.44|1g3vh6r90d7fsc54u...|          A|2019-05-22 06:08:32|\n",
      "|0001459cc41968df0...|      4|     19|     0.88|jlo86izb9mf753gux...|          A|2019-05-20 20:15:38|\n",
      "|00014b18aab2b376e...|     17|     11|     0.77|tz1sm07xa3gpcl6k4...|          B|2019-05-25 03:34:51|\n",
      "|0001a5d00032b7ea6...|     41|     11|     0.69|        g2um56hx9a8c|          A|2019-05-24 08:04:16|\n",
      "|0001b5eca5e2e80a5...|      6|     13|     0.47|    4av905nilj3mtz7w|          B|2019-05-23 22:13:58|\n",
      "|0001e2ec36e065321...|      5|      4|     0.85|          g7wsnh91e6|          A|2019-05-28 10:10:39|\n",
      "|000219f709ddd6ba0...|      4|      5|     1.00|         utxwpsz81fi|          B|2019-05-17 23:51:18|\n",
      "+--------------------+-------+-------+---------+--------------------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "# 方式1: 调DataFrame的方法\n",
    "df_time_filtered = df.filter((df.LOGTIME >= t1) & (df.LOGTIME < t2))\n",
    "df_time_filtered.coalesce(50).persist(StorageLevel.MEMORY_AND_DISK)  # 分区数=50 work\n",
    "print(df_time_filtered.count())\n",
    "%time df_time_filtered.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211410\n",
      "Wall time: 24.9 s\n"
     ]
    }
   ],
   "source": [
    "# 方式2: 注册该DataFrame为临时SQL表, 写sql实现\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df_time_filtered = spark_session.sql(\"\"\"\n",
    "    SELECT * FROM df WHERE LOGTIME BETWEEN '%s' AND '%s' ORDER BY LOGTIME ASC\n",
    "\"\"\" % (t1, t2))\n",
    "# 这种方式下Spark会先做一个执行计划, 然后调用Catalyst Optimizer, 比较稳定, 一般不会出GC的问题\n",
    "df_time_filtered.coalesce(25).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "%time print(df_time_filtered.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_filtered.show(10)  # show依然存在问题, 运行不出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.98 s\n",
      "+--------------------+---------+----------+--------------------+-----------+----------+\n",
      "|               LOGID|TOTAL_NUM|   S_TOTAL|           ITEM_CODE|STATUS_FLAG|   LOGDATE|\n",
      "+--------------------+---------+----------+--------------------+-----------+----------+\n",
      "|000005acf636cea98...|       21| 23.170732|5e0bqjonw8kmzya67...|          B|2019-05-18|\n",
      "|00002963bfc216bfa...|       31|600.000000|uo9dx5rhbp26kqwyc...|          B|2019-05-13|\n",
      "|0000bc5270286f7a7...|       46|258.333333|           hug4nvkwc|          B|2019-06-02|\n",
      "|000111c6c2c36620c...|       53| 77.272727|1g3vh6r90d7fsc54u...|          A|2019-05-22|\n",
      "|0001459cc41968df0...|       23|  4.545455|jlo86izb9mf753gux...|          A|2019-05-20|\n",
      "|00014b18aab2b376e...|       28| 22.077922|tz1sm07xa3gpcl6k4...|          B|2019-05-25|\n",
      "|0001a5d00032b7ea6...|       52| 59.420290|        g2um56hx9a8c|          A|2019-05-24|\n",
      "|0001b5eca5e2e80a5...|       19| 12.765957|    4av905nilj3mtz7w|          B|2019-05-23|\n",
      "|0001e2ec36e065321...|        9|  5.882353|          g7wsnh91e6|          A|2019-05-28|\n",
      "|000219f709ddd6ba0...|        9|  4.000000|         utxwpsz81fi|          B|2019-05-17|\n",
      "+--------------------+---------+----------+--------------------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "### 字段运算\n",
    "# 尽量使用方式2(写SQL实现运算和操作)\n",
    "df_time_filtered.createOrReplaceTempView(\"df_time_filtered\")\n",
    "df_smry = spark_session.sql(\"\"\"\n",
    "    SELECT \n",
    "        LOGID,\n",
    "        (ACQ_NUM + DEQ_NUM) AS TOTAL_NUM,  -- 简单加法\n",
    "        ACQ_NUM / ACQ_SHARE AS S_TOTAL,   -- 简单乘法\n",
    "        ITEM_CODE,\n",
    "        STATUS_FLAG,\n",
    "        DATE(LOGTIME) AS LOGDATE  -- datetime转为日期\n",
    "    FROM df_time_filtered\n",
    "\"\"\")\n",
    "df_smry.coalesce(25).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "%time df_smry.count()\n",
    "%time df_smry.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聚合操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.3 s\n",
      "+----------+-----------+-------------+------------------+-------------+--------------+\n",
      "|   LOGDATE|STATUS_FLAG|SUM_TOTAL_NUM|     AVG_TOTAL_NUM|  SUM_S_TOTAL|   AVG_S_TOTAL|\n",
      "+----------+-----------+-------------+------------------+-------------+--------------+\n",
      "|2019-05-06|          A|        58107| 32.15661317100166|220778.488848|122.6547160267|\n",
      "|2019-05-25|          B|       120847|32.148709763234905|393668.652214|105.0904036877|\n",
      "|2019-05-23|          B|       118127|31.857335490830636|438589.807326|118.7624715207|\n",
      "|2019-06-01|          A|       124614| 32.19168173598553|407792.589233|105.9752051021|\n",
      "|2019-05-13|          A|       120121|31.694195250659632|386811.079153|102.6841197645|\n",
      "|2019-05-20|          B|       121877|32.089784096893105|466899.724610|123.5838339359|\n",
      "|2019-05-10|          B|       122268| 32.03248624574273|427691.300044|112.9068901911|\n",
      "|2019-06-06|          B|        65058|31.844346549192363|252385.952369|124.3280553542|\n",
      "|2019-05-27|          B|       120400| 32.33951114692452|394622.493156|106.5107943741|\n",
      "|2019-05-15|          A|       121456|31.936891927425716|449173.986629|118.8290969918|\n",
      "+----------+-----------+-------------+------------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "# 依旧使用方式2实现\n",
    "# 按日期, STATUS_FLAG聚合; 计算衍生字段TOTAL_NUM和S_TOTAL的每日每类和与均值, 以及样本数\n",
    "df_smry.createOrReplaceTempView(\"df_smry\")\n",
    "df_time_filtered.unpersist()  # 不用的DataFrame要及时unpersist\n",
    "df_groupby_date_status_flag = spark_session.sql(\"\"\"\n",
    "    SELECT\n",
    "        LOGDATE,\n",
    "        STATUS_FLAG,\n",
    "        SUM(TOTAL_NUM) AS SUM_TOTAL_NUM,\n",
    "        AVG(TOTAL_NUM) AS AVG_TOTAL_NUM,\n",
    "        SUM(S_TOTAL) AS SUM_S_TOTAL,\n",
    "        AVG(S_TOTAL) AS AVG_S_TOTAL\n",
    "    FROM df_smry\n",
    "    GROUP BY LOGDATE, STATUS_FLAG\n",
    "\"\"\")\n",
    "df_groupby_date_status_flag.coalesce(25).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "%time df_groupby_date_status_flag.count()\n",
    "%time df_groupby_date_status_flag.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚合完成的结果写回数据库\n",
    "res_table = \"test.grouped_result\"\n",
    "%time df_groupby_date_status_flag.write.jdbc(url, res_table, properties=properties)\n",
    "# 这里的性能也有待提升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
