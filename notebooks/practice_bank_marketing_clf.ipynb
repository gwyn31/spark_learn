{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame as SparkDataFrame\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "TYPE_MAP = {  # 定义数据类型映射\n",
    "    # 下面的类均来自pyspark.sql.types\n",
    "    \"int\": IntegerType,\n",
    "    \"bigint\": IntegerType,\n",
    "    \"float\": FloatType,\n",
    "    \"double\": DoubleType,\n",
    "    \"string\": StringType,\n",
    "    \"decimal\": DecimalType,\n",
    "    \"bool\": BooleanType,\n",
    "    \"date\": DateType,\n",
    "    \"datetime\": TimestampType\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "### 初始化spark session\n",
    "spark_session = SparkSession.builder.master(\"local[4]\").appName(\"FE:bank-loan\").getOrCreate()\n",
    "print(spark_session.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 关于schema\n",
    "\"\"\"\n",
    "Note:\n",
    "除了预先定义schema, 还可以:\n",
    "1. 让spark自行推断schema(在read时传入inferschema='true'), 会造成较大的计算开销;\n",
    "2. 另一种做法是先全部读成文本, 随后再进行类型转换(比较safe, 但后面的工作比较麻烦)\n",
    "\"\"\"\n",
    "# 加载定义好的schema文件\n",
    "fields = []\n",
    "discrete_fields = [\"id\"]\n",
    "continuous_fields = [\"id\"]\n",
    "with open(\"D:/python_projects/spark_learn/data/bank-full-schema.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        name, data_type = line.strip().split(\",\")\n",
    "        fields.append(StructField(name, TYPE_MAP.get(data_type)(), nullable=True))\n",
    "        if name != \"id\":\n",
    "            if data_type == \"string\":\n",
    "                discrete_fields.append(name)\n",
    "            else:\n",
    "                continuous_fields.append(name)\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "| id|age|         job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|  y|\n",
      "+---+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "|  1| 58|  management| married| tertiary|     no| 2143.0|    yes|  no|unknown|  5|  may|   261.0|       1|   -1|       0| unknown| no|\n",
      "|  2| 44|  technician|  single|secondary|     no|   29.0|    yes|  no|unknown|  5|  may|   151.0|       1|   -1|       0| unknown| no|\n",
      "|  3| 33|entrepreneur| married|secondary|     no|    2.0|    yes| yes|unknown|  5|  may|    76.0|       1|   -1|       0| unknown| no|\n",
      "|  4| 47| blue-collar| married|  unknown|     no| 1506.0|    yes|  no|unknown|  5|  may|    92.0|       1|   -1|       0| unknown| no|\n",
      "|  5| 33|     unknown|  single|  unknown|     no|    1.0|     no|  no|unknown|  5|  may|   198.0|       1|   -1|       0| unknown| no|\n",
      "|  6| 35|  management| married| tertiary|     no|  231.0|    yes|  no|unknown|  5|  may|   139.0|       1|   -1|       0| unknown| no|\n",
      "|  7| 28|  management|  single| tertiary|     no|  447.0|    yes| yes|unknown|  5|  may|   217.0|       1|   -1|       0| unknown| no|\n",
      "|  8| 42|entrepreneur|divorced| tertiary|    yes|    2.0|    yes|  no|unknown|  5|  may|   380.0|       1|   -1|       0| unknown| no|\n",
      "|  9| 58|     retired| married|  primary|     no|  121.0|    yes|  no|unknown|  5|  may|    50.0|       1|   -1|       0| unknown| no|\n",
      "| 10| 43|  technician|  single|secondary|     no|  593.0|    yes|  no|unknown|  5|  may|    55.0|       1|   -1|       0| unknown| no|\n",
      "+---+---+------------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 加载数据集\n",
    "filepath = \"D:/python_projects/spark_learn/data/bank-full.csv\"\n",
    "df = spark_session.read.csv(filepath, schema=schema, sep=\",\", header=\"false\")\n",
    "# 这里python的传参更加直观, 也可以像scala一样使用.options(传入dict)\n",
    "# 注意bool型的参数要以字符串的形式传入\n",
    "df.show(10)\n",
    "# 在目前的storage level上将该数据集注册为一张表便于以后进行查询\n",
    "df.createOrReplaceTempView(\"bank_main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---+--------+--------+-----+--------+--------+------------+--------------+------------+------------+---------+------------+----------+-------------+------+\n",
      "| id|age|balance|day|duration|campaign|pdays|previous|job_code|marital_code|education_code|default_code|housing_code|loan_code|contact_code|month_code|poutcome_code|y_code|\n",
      "+---+---+-------+---+--------+--------+-----+--------+--------+------------+--------------+------------+------------+---------+------------+----------+-------------+------+\n",
      "|  1| 58| 2143.0|  5|   261.0|       1|   -1|       0|     1.0|         0.0|           1.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "|  2| 44|   29.0|  5|   151.0|       1|   -1|       0|     2.0|         1.0|           0.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "|  3| 33|    2.0|  5|    76.0|       1|   -1|       0|     7.0|         0.0|           0.0|         0.0|         0.0|      1.0|         1.0|       0.0|          0.0|   0.0|\n",
      "|  4| 47| 1506.0|  5|    92.0|       1|   -1|       0|     0.0|         0.0|           3.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "|  5| 33|    1.0|  5|   198.0|       1|   -1|       0|    11.0|         1.0|           3.0|         0.0|         1.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "|  6| 35|  231.0|  5|   139.0|       1|   -1|       0|     1.0|         0.0|           1.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "|  7| 28|  447.0|  5|   217.0|       1|   -1|       0|     1.0|         1.0|           1.0|         0.0|         0.0|      1.0|         1.0|       0.0|          0.0|   0.0|\n",
      "|  8| 42|    2.0|  5|   380.0|       1|   -1|       0|     7.0|         2.0|           1.0|         1.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "|  9| 58|  121.0|  5|    50.0|       1|   -1|       0|     5.0|         0.0|           2.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "| 10| 43|  593.0|  5|    55.0|       1|   -1|       0|     2.0|         1.0|           0.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "| 11| 41|  270.0|  5|   222.0|       1|   -1|       0|     3.0|         2.0|           0.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "| 12| 29|  390.0|  5|   137.0|       1|   -1|       0|     3.0|         1.0|           0.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "| 13| 53|    6.0|  5|   517.0|       1|   -1|       0|     2.0|         0.0|           0.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "| 14| 58|   71.0|  5|    71.0|       1|   -1|       0|     2.0|         0.0|           3.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "| 15| 57|  162.0|  5|   174.0|       1|   -1|       0|     4.0|         0.0|           0.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "| 16| 51|  229.0|  5|   353.0|       1|   -1|       0|     5.0|         0.0|           2.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "| 17| 45|   13.0|  5|    98.0|       1|   -1|       0|     3.0|         1.0|           3.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "| 18| 57|   52.0|  5|    38.0|       1|   -1|       0|     0.0|         0.0|           2.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "| 19| 60|   60.0|  5|   219.0|       1|   -1|       0|     5.0|         0.0|           2.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "| 20| 33|    0.0|  5|    54.0|       1|   -1|       0|     4.0|         0.0|           0.0|         0.0|         0.0|      0.0|         1.0|       0.0|          0.0|   0.0|\n",
      "+---+---+-------+---+--------+--------+-----+--------+--------+------------+--------------+------------+------------+---------+------------+----------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对离散型特征进行编码\n",
    "df_numerics = df.select(*continuous_fields)\n",
    "df_discrete = df.select(*discrete_fields)\n",
    "transformed_cols = []\n",
    "for col in discrete_fields:\n",
    "    if col != \"id\":\n",
    "        label_encoder = StringIndexer(inputCol=col, outputCol=\"%s_code\" % col)  # 初始化encoder\n",
    "        col_df = df_discrete.select(\"id\", col)  # 选中一个离散特征\n",
    "        transformed_cols.append(\"%s_code\" % col)\n",
    "        encoded = label_encoder.fit(col_df).transform(col_df).select([\"id\", \"%s_code\" % col])  # 进行编码\n",
    "        df_numerics = df_numerics.join(encoded, \"id\")  # 与原来所有数值特征拼接\n",
    "df_numerics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\python_projects\\jupyter_container\\venv\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36m_set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python_projects\\jupyter_container\\venv\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36mtoString\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Could not convert %s to string type\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not convert <class 'list'> to string type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2863ce62b506>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfeats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_numerics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"y_code\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'y_code'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\python_projects\\jupyter_container\\venv\\lib\\site-packages\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python_projects\\jupyter_container\\venv\\lib\\site-packages\\pyspark\\ml\\classification.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, featuresCol, labelCol, predictionCol, probabilityCol, rawPredictionCol, maxDepth, maxBins, minInstancesPerNode, minInfoGain, maxMemoryInMB, cacheNodeIds, checkpointInterval, impurity, seed)\u001b[0m\n\u001b[0;32m    984\u001b[0m                          impurity=\"gini\")\n\u001b[0;32m    985\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mkeyword_only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python_projects\\jupyter_container\\venv\\lib\\site-packages\\pyspark\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python_projects\\jupyter_container\\venv\\lib\\site-packages\\pyspark\\ml\\classification.py\u001b[0m in \u001b[0;36msetParams\u001b[1;34m(self, featuresCol, labelCol, predictionCol, probabilityCol, rawPredictionCol, maxDepth, maxBins, minInstancesPerNode, minInfoGain, maxMemoryInMB, cacheNodeIds, checkpointInterval, impurity, seed)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         \"\"\"\n\u001b[0;32m   1003\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python_projects\\jupyter_container\\venv\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py\u001b[0m in \u001b[0;36m_set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Invalid param value given for param \"%s\". %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid param value given for param \"featuresCol\". Could not convert <class 'list'> to string type"
     ]
    }
   ],
   "source": [
    "### 建立决策树模型\n",
    "# 正在学习中(2020-02-14)\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
